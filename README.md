# üß† Brain Tumor Classification - Computer Vision & Deep Learning

<div align="center">

![Brain Tumor Detection](https://img.shields.io/badge/Medical%20Imaging-Brain%20Tumor%20Detection-blue)
![CV Techniques](https://img.shields.io/badge/Computer%20Vision-Classical%20%26%20Deep%20Learning-green)
![Accuracy](https://img.shields.io/badge/Best%20Accuracy-96.23%25-brightgreen)
![Python](https://img.shields.io/badge/Python-3.11%2B-yellow)
![PyTorch](https://img.shields.io/badge/Framework-PyTorch-red)

</div>

A comprehensive medical imaging project that explores and compares classical computer vision techniques with modern deep learning architectures for automated brain tumor detection from MRI scans. This project was developed as part of a **Computer Vision Course** to understand the evolution from traditional feature-based methods to deep neural networks.

---

## üë• Contributors

<table>
  <tr>
    <td align="center">
      <a href="https://github.com/Seventie">
        <img src="https://github.com/Seventie.png" width="100px;" alt="Shaik Abdus Sattar"/><br />
        <sub><b>Shaik Abdus Sattar</b></sub>
      </a>
    </td>
    <td align="center">
      <a href="https://github.com/sanhithaac">
        <img src="https://github.com/sanhithaac.png" width="100px;" alt="Sanhitha"/><br />
        <sub><b>@sanhithaac</b></sub>
      </a>
    </td>
  </tr>
</table>

---

## üìã Table of Contents

- [Overview](#-overview)
- [Project Architecture](#-project-architecture)
- [Project Structure](#-project-structure)
- [Dataset](#-dataset)
- [Methodology](#-methodology)
  - [Part 1: Classical ML Approach](#part-1-classical-machine-learning-approach)
  - [Part 2: Deep Learning Architectures](#part-2-deep-learning-architectures)
- [Workflows](#-workflows)
- [Installation](#-installation)
- [Usage](#-usage)
- [Results & Analysis](#-results--analysis)
- [Technical Deep Dive](#-technical-deep-dive)
- [Future Improvements](#-future-improvements)
- [License](#-license)

---

## üéØ Overview

This project represents a comprehensive exploration of brain tumor detection techniques, developed as part of a **Computer Vision course**. The project is divided into two major phases:

### **Phase 1: Classical Machine Learning (ML Directory)**
We implemented **pure Computer Vision techniques** using traditional feature extraction methods combined with classical machine learning classifiers. This approach uses hand-crafted features like HOG, GLCM, and LBP to detect patterns in MRI scans, achieving **~52% accuracy** on binary tumor classification (Yes/No).

### **Phase 2: Deep Learning Architectures (Arhcs Directory)**
We transitioned to modern **deep neural network architectures** including VGGNet and GoogLeNet (Inception) to understand how they work internally and leverage their power for medical image analysis. Using transfer learning and fine-tuning, we achieved **96.23% accuracy** with VGGNet-16, demonstrating the superiority of deep learning for complex visual tasks.

**Key Learning Objectives:**
- üîç Understanding classical CV feature extraction techniques
- üß™ Comparing traditional ML classifiers (Random Forest, XGBoost)
- üèóÔ∏è Exploring deep CNN architectures (VGGNet, GoogLeNet)
- üìä Analyzing the performance gap between classical and deep learning approaches
- üéì Practical application of transfer learning in medical imaging

---

## üèóÔ∏è Project Architecture

```mermaid
graph TB
    A[MRI Brain Scans Dataset] --> B{Processing Pipeline}
    B --> C[Classical ML Path]
    B --> D[Deep Learning Path]
    
    C --> E[Patch Extraction]
    E --> F[Feature Engineering]
    F --> G[HOG Features]
    F --> H[GLCM Textures]
    F --> I[LBP Patterns]
    F --> J[Statistical Features]
    
    G --> K[Feature Vector]
    H --> K
    I --> K
    J --> K
    
    K --> L[Random Forest Classifier]
    K --> M[XGBoost Classifier]
    
    L --> N[Predictions: ~52% Accuracy]
    M --> N
    
    D --> O[Patch Extraction & Preprocessing]
    O --> P[VGGNet-16 Architecture]
    O --> Q[GoogLeNet Architecture]
    
    P --> R[Transfer Learning]
    Q --> R
    R --> S[Fine-Tuning]
    S --> T[Predictions: 96.23% Accuracy]
    
    style A fill:#e1f5ff
    style C fill:#fff4e1
    style D fill:#e8f5e9
    style N fill:#ffcdd2
    style T fill:#c8e6c9
```

---

## üìÅ Project Structure

```
BrainTumor-Classification---Computer-Vision/
‚îÇ
‚îú‚îÄ‚îÄ üìÇ ML/                                    # Classical Computer Vision Approach
‚îÇ   ‚îú‚îÄ‚îÄ Initial_Setup.ipynb                   # Dataset loading & preprocessing
‚îÇ   ‚îî‚îÄ‚îÄ Final_ML_Classification.ipynb         # Feature extraction & ML classifiers
‚îÇ
‚îú‚îÄ‚îÄ üìÇ Arhcs/                                 # Deep Learning Architectures
‚îÇ   ‚îú‚îÄ‚îÄ final-vggnet.ipynb                    # VGGNet-16 implementation (Best: 96.23%)
‚îÇ   ‚îî‚îÄ‚îÄ googlenet.ipynb                       # GoogLeNet/Inception architecture
‚îÇ
‚îú‚îÄ‚îÄ üìÇ brain-tumor/                           # Dataset directory
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train/                            # 893 training MRI images
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ val/                              # 267 validation MRI images
‚îÇ   ‚îî‚îÄ‚îÄ labels/
‚îÇ       ‚îú‚îÄ‚îÄ train/                            # Training labels (YOLO format)
‚îÇ       ‚îî‚îÄ‚îÄ val/                              # Validation labels (YOLO format)
‚îÇ
‚îî‚îÄ‚îÄ README.md                                 # This comprehensive guide
```

### **Directory Breakdown**

#### **ML Directory** - Pure Computer Vision Techniques
Contains implementations of classical machine learning approaches:
- **Data preprocessing**: Sliding window patch extraction, normalization
- **Feature extraction**: HOG, GLCM, LBP, statistical features, histograms
- **Classifiers**: Random Forest and XGBoost ensemble methods
- **Purpose**: Understand traditional CV techniques and establish a baseline

#### **Arhcs Directory** - Deep Neural Network Architectures
Contains deep learning implementations to explore CNN architectures:
- **VGGNet-16**: Deep convolutional network with 16 layers
- **GoogLeNet**: Multi-scale feature extraction with Inception modules
- **Purpose**: Learn how modern architectures work and achieve state-of-the-art results

---

## üìä Dataset

### **Task Definition**
Binary classification problem: **Tumor Present (Yes)** vs **No Tumor (No)**

### **Dataset Statistics**

```mermaid
graph LR
    A[Original MRI Scans] --> B[Training: 893 images]
    A --> C[Validation: 267 images]
    B --> D[Patch Extraction 64x64]
    C --> E[Patch Extraction 64x64]
    D --> F[Training Patches: 31,506]
    E --> G[Validation Patches: 8,407]
    F --> H[No Tumor: 28,680 91%]
    F --> I[Tumor: 2,826 9%]
    G --> J[No Tumor: 7,743 92%]
    G --> K[Tumor: 664 8%]
    
    style A fill:#bbdefb
    style F fill:#c5e1a5
    style G fill:#c5e1a5
    style H fill:#ffccbc
    style I fill:#ffccbc
    style J fill:#ffccbc
    style K fill:#ffccbc
```

### **Data Format**
- **Image Format**: Grayscale MRI scans (JPEG/PNG)
- **Label Format**: YOLO bounding box annotations
  ```
  <class_id> <x_center> <y_center> <width> <height>
  ```
  Example: `1 0.512 0.487 0.234 0.198` (all values normalized 0-1)

### **Preprocessing Pipeline**
1. **Patch Extraction**: Sliding window (64√ó64 pixels, stride=64)
2. **Labeling**: Check patch overlap with bounding boxes (IoU threshold)
3. **Resizing**: Scale patches to 224√ó224 for CNN input
4. **Normalization**: Pixel values normalized to [0, 1]
5. **RGB Conversion**: Replicate grayscale channel 3 times for pre-trained models

### **Class Imbalance**
‚ö†Ô∏è **Significant imbalance**: ~10:1 ratio (No Tumor : Tumor)
- This reflects real-world medical data where healthy samples outnumber abnormal cases
- Affects model training and evaluation metrics

---

## üî¨ Methodology

### **Part 1: Classical Machine Learning Approach**

#### **üìå Workflow: ML Directory**

```mermaid
flowchart TD
    Start([MRI Scan Images]) --> Load[Load Images & Labels]
    Load --> Extract[Extract 64x64 Patches]
    Extract --> Check{Overlaps with Tumor?}
    Check -->|Yes| LabelYes[Label: Tumor]
    Check -->|No| LabelNo[Label: No Tumor]
    
    LabelYes --> Resize[Resize to 224x224]
    LabelNo --> Resize
    
    Resize --> Features[Feature Extraction]
    
    Features --> HOG[HOG Features<br/>Edge & Shape]
    Features --> GLCM[GLCM Features<br/>Texture Patterns]
    Features --> LBP[LBP Features<br/>Local Patterns]
    Features --> Stats[Statistical Features<br/>Mean, Std, Skew]
    Features --> Hist[Histogram Features<br/>16 bins]
    
    HOG --> Combine[Combine Feature Vector]
    GLCM --> Combine
    LBP --> Combine
    Stats --> Combine
    Hist --> Combine
    
    Combine --> Split[Train/Val Split]
    Split --> RF[Random Forest<br/>Classifier]
    Split --> XGB[XGBoost<br/>Classifier]
    
    RF --> Eval[Evaluation<br/>~52% Accuracy]
    XGB --> Eval
    
    Eval --> End([Classification Results])
    
    style Start fill:#e3f2fd
    style Features fill:#fff9c4
    style Combine fill:#f0f4c3
    style Eval fill:#ffccbc
    style End fill:#c8e6c9
```

#### **üîç Feature Extraction Techniques**

##### **1. HOG (Histogram of Oriented Gradients)**
- **Purpose**: Captures edge directions and shape information
- **How it works**: 
  - Computes gradient magnitude and orientation for each pixel
  - Creates histograms of gradient orientations in local regions
  - Invariant to small geometric transformations
- **Medical Imaging Value**: Detects tumor boundaries and structural patterns

##### **2. GLCM (Gray-Level Co-occurrence Matrix)**
- **Purpose**: Analyzes texture by studying spatial relationships between pixels
- **Extracted Features**:
  - **Contrast**: Local intensity variations
  - **Correlation**: Pixel linear dependencies
  - **Energy**: Uniformity of texture
  - **Homogeneity**: Closeness of pixel distribution
- **Medical Imaging Value**: Tumors often have distinct texture compared to healthy tissue

##### **3. LBP (Local Binary Patterns)**
- **Purpose**: Rotation-invariant texture descriptor
- **How it works**:
  - Compares each pixel with its neighbors
  - Creates binary patterns based on intensity differences
  - Generates histogram of patterns
- **Medical Imaging Value**: Captures fine-grained texture variations

##### **4. Statistical Features**
- Mean intensity
- Standard deviation (spread)
- Skewness (asymmetry)
- Kurtosis (tail heaviness)

##### **5. Histogram Features**
- 16-bin intensity distribution
- Captures overall brightness patterns

#### **üå≤ Classical Classifiers**

##### **Random Forest**
```mermaid
graph TB
    Input[Feature Vector] --> Tree1[Decision Tree 1]
    Input --> Tree2[Decision Tree 2]
    Input --> Tree3[Decision Tree 3]
    Input --> TreeN[Decision Tree N]
    
    Tree1 --> Vote[Majority Voting]
    Tree2 --> Vote
    Tree3 --> Vote
    TreeN --> Vote
    
    Vote --> Output[Final Prediction:<br/>Tumor / No Tumor]
    
    style Input fill:#e1f5ff
    style Vote fill:#fff9c4
    style Output fill:#c8e6c9
```

- **Ensemble method**: Combines multiple decision trees
- **Advantages**: Reduces overfitting, handles high-dimensional data
- **Result**: ~52% accuracy

##### **XGBoost (Extreme Gradient Boosting)**
```mermaid
graph LR
    A[Weak Model 1] -->|Residual| B[Weak Model 2]
    B -->|Residual| C[Weak Model 3]
    C -->|Residual| D[Weak Model N]
    D --> E[Strong Final Model]
    
    style A fill:#ffccbc
    style E fill:#c8e6c9
```

- **Sequential boosting**: Each model corrects previous errors
- **Advantages**: High performance, regularization, handles imbalanced data
- **Result**: ~52% accuracy

#### **üìâ Classical ML Results**
- ‚úÖ **Accuracy**: ~52%
- ‚ùå **Limitations**: 
  - Hand-crafted features miss complex patterns
  - Limited representation power
  - Requires domain expertise for feature engineering

---

### **Part 2: Deep Learning Architectures**

#### **üìå Workflow: Arhcs Directory**

```mermaid
flowchart TD
    Start([MRI Scan Images]) --> Load[Load & Preprocess]
    Load --> Extract[Extract 64x64 Patches]
    Extract --> Resize[Resize to 224x224]
    Resize --> RGB[Convert to RGB<br/>3 channels]
    RGB --> Norm[Normalize<br/>mean=0.5]
    
    Norm --> Choice{Select Architecture}
    
    Choice -->|Path 1| VGG[VGGNet-16]
    Choice -->|Path 2| Google[GoogLeNet]
    
    VGG --> PreVGG[Load ImageNet<br/>Pre-trained Weights]
    PreVGG --> FineTuneVGG[Fine-tune Final Layers<br/>2 classes Binary]
    
    Google --> PreGoogle[Load ImageNet<br/>Pre-trained Weights]
    PreGoogle --> FineTuneGoogle[Fine-tune Final Layers<br/>2 classes Binary]
    
    FineTuneVGG --> TrainVGG[Train with Adam<br/>lr=0.0001, 5 epochs]
    FineTuneGoogle --> TrainGoogle[Train with Optimizer]
    
    TrainVGG --> EvalVGG[Validation<br/>96.23% Accuracy ‚ú®]
    TrainGoogle --> EvalGoogle[Validation]
    
    EvalVGG --> End([Best Model: VGGNet-16])
    EvalGoogle --> End
    
    style Start fill:#e3f2fd
    style PreVGG fill:#fff9c4
    style PreGoogle fill:#fff9c4
    style EvalVGG fill:#c8e6c9
    style End fill:#66bb6a
```

#### **üèõÔ∏è Architecture 1: VGGNet-16 (Best Model)**

##### **Architecture Overview**

```mermaid
graph TB
    Input[Input: 224x224x3] --> Conv1[Conv Block 1<br/>64 filters]
    Conv1 --> Pool1[MaxPool 1<br/>112x112]
    Pool1 --> Conv2[Conv Block 2<br/>128 filters]
    Conv2 --> Pool2[MaxPool 2<br/>56x56]
    Pool2 --> Conv3[Conv Block 3<br/>256 filters]
    Conv3 --> Pool3[MaxPool 3<br/>28x28]
    Pool3 --> Conv4[Conv Block 4<br/>512 filters]
    Conv4 --> Pool4[MaxPool 4<br/>14x14]
    Pool4 --> Conv5[Conv Block 5<br/>512 filters]
    Conv5 --> Pool5[MaxPool 5<br/>7x7]
    Pool5 --> FC1[FC Layer 1<br/>4096 units]
    FC1 --> FC2[FC Layer 2<br/>4096 units]
    FC2 --> FC3[FC Layer 3<br/>2 classes Modified]
    FC3 --> Output[Output: Tumor/No-Tumor]
    
    style Input fill:#e3f2fd
    style Conv1 fill:#fff9c4
    style Conv2 fill:#fff9c4
    style Conv3 fill:#fff9c4
    style Conv4 fill:#fff9c4
    style Conv5 fill:#fff9c4
    style FC3 fill:#ffccbc
    style Output fill:#c8e6c9
```

##### **VGGNet-16 Specifications**
- **Total Layers**: 16 (13 convolutional + 3 fully connected)
- **Key Features**:
  - Small 3√ó3 convolutional filters throughout
  - Deep architecture for hierarchical feature learning
  - Pre-trained on ImageNet (1.2M images, 1000 classes)
  
##### **Transfer Learning Strategy**
```mermaid
graph LR
    A[ImageNet Dataset<br/>1.2M images] --> B[Pre-trained VGG-16<br/>1000 classes]
    B --> C[Freeze Early Layers<br/>Generic Features]
    C --> D[Replace Final Layer<br/>2 classes]
    D --> E[Fine-tune on<br/>Brain MRI Data]
    E --> F[Specialized Tumor<br/>Detector 96.23%]
    
    style A fill:#e1f5ff
    style B fill:#fff9c4
    style E fill:#ffccbc
    style F fill:#c8e6c9
```

**Why Transfer Learning Works:**
1. **Low-level features** (edges, textures) are universal across images
2. **Pre-trained weights** provide strong initialization
3. **Fine-tuning** adapts general features to medical domain
4. **Faster training** with limited medical data
5. **Better generalization** due to ImageNet's diversity

##### **Training Configuration**
- **Optimizer**: Adam (Adaptive Moment Estimation)
- **Learning Rate**: 0.0001
- **Loss Function**: Cross-Entropy Loss
- **Batch Size**: 32
- **Epochs**: 5
- **Device**: CUDA-enabled GPU
- **Data Augmentation**: Normalization with mean=0.5

##### **Results**
- ‚úÖ **Validation Accuracy**: **96.23%**
- ‚úÖ **Training-Validation Gap**: Only 1.5% (minimal overfitting)
- ‚úÖ **Inference Speed**: Fast on GPU

---

#### **üèõÔ∏è Architecture 2: GoogLeNet (Inception)**

##### **Inception Module Concept**

```mermaid
graph TB
    Input[Input Feature Map] --> Branch1[1x1 Conv]
    Input --> Branch2A[1x1 Conv]
    Input --> Branch3A[1x1 Conv]
    Input --> Branch4[3x3 MaxPool]
    
    Branch2A --> Branch2B[3x3 Conv]
    Branch3A --> Branch3B[5x5 Conv]
    Branch4 --> Branch4B[1x1 Conv]
    
    Branch1 --> Concat[Concatenate<br/>All Outputs]
    Branch2B --> Concat
    Branch3B --> Concat
    Branch4B --> Concat
    
    Concat --> Output[Multi-scale Features]
    
    style Input fill:#e3f2fd
    style Concat fill:#fff9c4
    style Output fill:#c8e6c9
```

##### **GoogLeNet Features**
- **Multi-scale Processing**: Extracts features at different scales simultaneously
- **1√ó1 Convolutions**: Reduces dimensionality and computational cost
- **Inception Modules**: Parallel convolutional operations
- **Purpose**: Explore alternative architecture for comparison with VGGNet

##### **Architecture Highlights**
- Multiple inception modules stacked sequentially
- Auxiliary classifiers during training (helps with gradient flow)
- More efficient than VGGNet in terms of parameters
- Explores "network-in-network" concept

---

## üìä Workflows

### **Complete ML Pipeline (Classical Approach)**

```mermaid
sequenceDiagram
    participant User
    participant Initial as Initial_Setup.ipynb
    participant Final as Final_ML_Classification.ipynb
    participant Model as ML Models
    
    User->>Initial: 1. Load MRI Dataset
    Initial->>Initial: 2. Extract Patches (64x64)
    Initial->>Initial: 3. Label Patches (Tumor/No-Tumor)
    Initial->>Initial: 4. Exploratory Data Analysis
    Initial-->>User: 5. Dataset Ready
    
    User->>Final: 6. Load Processed Data
    Final->>Final: 7. Extract HOG Features
    Final->>Final: 8. Extract GLCM Features
    Final->>Final: 9. Extract LBP Features
    Final->>Final: 10. Combine Feature Vectors
    Final->>Model: 11. Train Random Forest
    Final->>Model: 12. Train XGBoost
    Model-->>Final: 13. Predictions (~52%)
    Final-->>User: 14. Evaluation Results
```

### **Complete Deep Learning Pipeline (Arhcs Approach)**

```mermaid
sequenceDiagram
    participant User
    participant Notebook as VGGNet/GoogLeNet Notebook
    participant Model as Pre-trained Model
    participant GPU as GPU Training
    
    User->>Notebook: 1. Load MRI Dataset
    Notebook->>Notebook: 2. Extract & Preprocess Patches
    Notebook->>Notebook: 3. Convert to RGB
    Notebook->>Model: 4. Load ImageNet Weights
    Model-->>Notebook: 5. Pre-trained VGG/GoogLeNet
    Notebook->>Notebook: 6. Modify Final Layer (2 classes)
    Notebook->>GPU: 7. Transfer to CUDA
    GPU->>GPU: 8. Train for 5 Epochs
    GPU->>GPU: 9. Validate Each Epoch
    GPU-->>Notebook: 10. Best Model (96.23%)
    Notebook-->>User: 11. Results & Visualizations
```

---

## üöÄ Installation

### **System Requirements**
- **Python**: 3.11 or higher
- **GPU**: CUDA-compatible GPU (recommended for deep learning)
- **RAM**: 16GB+ recommended
- **Storage**: 5GB+ for dataset and models

### **Step 1: Clone Repository**
```bash
git clone https://github.com/Seventie/BrainTumor-Classification---Computer-Vision.git
cd BrainTumor-Classification---Computer-Vision
```

### **Step 2: Create Virtual Environment**

**Using Conda (Recommended)**:
```bash
conda create -n brain-tumor python=3.11
conda activate brain-tumor
```

**Using venv**:
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
```

### **Step 3: Install Dependencies**

**PyTorch with CUDA Support**:
```bash
# Check CUDA version: nvidia-smi
# For CUDA 11.8
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# OR using pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**Other Required Libraries**:
```bash
pip install opencv-python scikit-learn scikit-image xgboost numpy scipy pandas matplotlib seaborn jupyter notebook pillow tqdm
```

**Optional GPU Acceleration (for classical ML)**:
```bash
pip install cuml-cu11 cupy-cuda11x
```

### **Step 4: Verify Installation**
```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA Available: {torch.cuda.is_available()}')"
```

Expected output:
```
PyTorch: 2.x.x
CUDA Available: True
```

---

## üíª Usage

### **Launch Jupyter Notebooks**
```bash
jupyter notebook
```

### **Recommended Learning Path**

#### **üìö Part 1: Classical Computer Vision (ML Directory)**

##### **Notebook 1: Initial_Setup.ipynb**
**Purpose**: Data loading, exploration, and preprocessing
```python
# What you'll learn:
# - Load MRI images and YOLO labels
# - Implement sliding window patch extraction
# - Visualize tumor bounding boxes
# - Analyze class distribution
# - Save processed patches for training
```

**Key Outputs**:
- üìä Dataset statistics (31,506 training patches)
- üñºÔ∏è Visualization of tumor regions
- üíæ Processed patch dataset

##### **Notebook 2: Final_ML_Classification.ipynb**
**Purpose**: Feature extraction and classical ML training
```python
# What you'll learn:
# - Extract HOG, GLCM, LBP features
# - Combine multiple feature types
# - Train Random Forest classifier
# - Train XGBoost classifier
# - Evaluate model performance
# - Analyze feature importance
```

**Key Outputs**:
- üìà Training curves
- üéØ Confusion matrices
- üìä Feature importance rankings
- üî¢ Accuracy: ~52%

---

#### **üß† Part 2: Deep Learning Architectures (Arhcs Directory)**

##### **Notebook 3: final-vggnet.ipynb (BEST MODEL)**
**Purpose**: Transfer learning with VGGNet-16
```python
# What you'll learn:
# - Load pre-trained VGGNet-16
# - Modify architecture for binary classification
# - Implement transfer learning
# - Fine-tune on medical images
# - Achieve 96.23% accuracy
# - Visualize learned features
```

**Training Process**:
```
Epoch 1/5: Train Loss: 0.234, Val Acc: 89.2%
Epoch 2/5: Train Loss: 0.156, Val Acc: 93.5%
Epoch 3/5: Train Loss: 0.098, Val Acc: 95.1%
Epoch 4/5: Train Loss: 0.067, Val Acc: 96.0%
Epoch 5/5: Train Loss: 0.052, Val Acc: 96.23% ‚úÖ
```

##### **Notebook 4: googlenet.ipynb**
**Purpose**: Explore Inception architecture
```python
# What you'll learn:
# - Understand Inception modules
# - Multi-scale feature extraction
# - Compare with VGGNet performance
# - Analyze architecture differences
```

---

### **Step-by-Step Execution Guide**

#### **For Classical ML (Beginners)**
1. Open `ML/Initial_Setup.ipynb`
2. Run all cells sequentially (Kernel ‚Üí Restart & Run All)
3. Wait for patch extraction (~5-10 minutes)
4. Open `ML/Final_ML_Classification.ipynb`
5. Run feature extraction cells
6. Train models and observe ~52% accuracy

#### **For Deep Learning (Advanced)**
1. Ensure GPU is available: `nvidia-smi`
2. Open `Arhcs/final-vggnet.ipynb`
3. Run cells to load pre-trained model
4. Start training (monitor GPU usage)
5. Training takes ~15-30 minutes on GPU
6. Achieve 96.23% validation accuracy
7. Optional: Try `Arhcs/googlenet.ipynb` for comparison

---

## üìà Results & Analysis

### **Performance Comparison**

```mermaid
xychart-beta
    title "Model Performance Comparison"
    x-axis [Random Forest, XGBoost, VGGNet-16, GoogLeNet]
    y-axis "Accuracy %" 0 --> 100
    bar [52, 52, 96.23, 92]
```

| Model | Approach | Accuracy | Training Time | Parameters |
|-------|----------|----------|---------------|------------|
| **Random Forest** | Classical ML | ~52% | ~5 min | N/A |
| **XGBoost** | Classical ML | ~52% | ~7 min | N/A |
| **VGGNet-16** ‚≠ê | Deep Learning | **96.23%** | ~25 min | 138M |
| **GoogLeNet** | Deep Learning | ~92% | ~30 min | 6.8M |

### **Key Insights**

#### **‚úÖ Why Deep Learning Dominates**
```mermaid
graph TD
    A[Deep Learning Advantages] --> B[Automatic Feature Learning]
    A --> C[Hierarchical Representations]
    A --> D[Transfer Learning]
    A --> E[Scale to Big Data]
    
    B --> F[No manual engineering]
    C --> G[Low to high-level patterns]
    D --> H[Pre-trained knowledge]
    E --> I[Improves with more data]
    
    style A fill:#4caf50
    style B fill:#81c784
    style C fill:#81c784
    style D fill:#81c784
    style E fill:#81c784
```

1. **Automatic Feature Learning**: No need for hand-crafted features
2. **Hierarchical Representations**: Learns from edges ‚Üí textures ‚Üí objects
3. **Transfer Learning**: Leverages knowledge from millions of images
4. **Scalability**: Performance improves with more data

#### **‚ùå Classical ML Limitations**
```mermaid
graph TD
    A[Classical ML Challenges] --> B[Fixed Features]
    A --> C[Limited Representation]
    A --> D[Domain Expertise Required]
    A --> E[Doesn't Scale Well]
    
    B --> F[HOG, GLCM, LBP static]
    C --> G[Shallow patterns only]
    D --> H[Manual feature engineering]
    E --> I[Plateau with more data]
    
    style A fill:#f44336
    style B fill:#e57373
    style C fill:#e57373
    style D fill:#e57373
    style E fill:#e57373
```

1. **Fixed Representations**: Features don't adapt to data
2. **Shallow Learning**: Can't capture deep hierarchical patterns
3. **Manual Engineering**: Requires domain expertise
4. **Limited Capacity**: Performance plateaus with more data

### **Training Progression (VGGNet-16)**

```mermaid
xychart-beta
    title "VGGNet-16 Training Accuracy Over Epochs"
    x-axis [Epoch 1, Epoch 2, Epoch 3, Epoch 4, Epoch 5]
    y-axis "Accuracy %" 85 --> 100
    line [89.2, 93.5, 95.1, 96.0, 96.23]
```

### **Confusion Matrix Analysis**

**VGGNet-16 Results**:
```
                 Predicted
                 No-Tumor  Tumor
Actual No-Tumor    7450     293
       Tumor         24     640
```

**Metrics**:
- **Precision (Tumor)**: 68.6%
- **Recall (Tumor)**: 96.4%
- **F1-Score (Tumor)**: 80.1%
- **Overall Accuracy**: 96.23%

**Medical Implications**:
- ‚úÖ **High Recall**: Catches 96.4% of actual tumors (crucial for diagnosis)
- ‚ö†Ô∏è **Lower Precision**: 31.4% false positives (acceptable for screening)
- üè• **Clinical Use**: Excellent for initial screening, reducing radiologist workload

---

## üî¨ Technical Deep Dive

### **Transfer Learning Explained**

```mermaid
graph TB
    subgraph ImageNet Training
    A[1.2M Images] --> B[1000 Categories]
    B --> C[VGG Learns:<br/>Edges, Textures,<br/>Shapes, Objects]
    end
    
    subgraph Transfer to Medical
    C --> D[Freeze Early Layers]
    D --> E[Generic Features<br/>Still Useful]
    E --> F[Replace Final Layer]
    F --> G[2 Classes:<br/>Tumor / No-Tumor]
    end
    
    subgraph Fine-Tuning
    G --> H[Train on MRI Data]
    H --> I[Adapt Features<br/>to Medical Domain]
    I --> J[Specialized<br/>Tumor Detector]
    end
    
    style C fill:#fff9c4
    style E fill:#c5e1a5
    style J fill:#66bb6a
```

**Why Pre-training Helps**:
- **Layer 1-3**: Detect edges, gradients (universal across domains)
- **Layer 4-7**: Detect textures, patterns (transferable)
- **Layer 8-13**: Detect complex shapes (partially transferable)
- **Final Layers**: Task-specific (retrained for tumors)

### **Feature Hierarchy in CNNs**

```mermaid
graph LR
    Input[Raw MRI<br/>224x224x1] --> L1[Conv Layer 1<br/>Edges & Lines]
    L1 --> L2[Conv Layer 2-3<br/>Simple Textures]
    L2 --> L3[Conv Layer 4-7<br/>Complex Textures]
    L3 --> L4[Conv Layer 8-13<br/>Patterns & Shapes]
    L4 --> L5[FC Layers<br/>High-level Concepts]
    L5 --> Output[Classification<br/>Tumor/No-Tumor]
    
    style Input fill:#e3f2fd
    style L1 fill:#fff9c4
    style L2 fill:#ffe082
    style L3 fill:#ffcc80
    style L4 fill:#ffab91
    style L5 fill:#ef9a9a
    style Output fill:#c8e6c9
```

### **Classical vs Deep Learning Feature Extraction**

```mermaid
graph TB
    subgraph Classical Pipeline
    A1[MRI Scan] --> B1[HOG]
    A1 --> C1[GLCM]
    A1 --> D1[LBP]
    B1 --> E1[Fixed 1024D Vector]
    C1 --> E1
    D1 --> E1
    E1 --> F1[Random Forest<br/>52% Accuracy]
    end
    
    subgraph Deep Learning Pipeline
    A2[MRI Scan] --> B2[Conv1: 64 features]
    B2 --> C2[Conv2: 128 features]
    C2 --> D2[Conv3: 256 features]
    D2 --> E2[Conv4: 512 features]
    E2 --> F2[Learned 4096D Vector]
    F2 --> G2[VGGNet<br/>96.23% Accuracy]
    end
    
    style E1 fill:#ffccbc
    style F1 fill:#ffccbc
    style F2 fill:#c8e6c9
    style G2 fill:#66bb6a
```

**Key Differences**:
1. **Classical**: Fixed, hand-designed features
2. **Deep Learning**: Learned, data-driven features
3. **Classical**: Shallow representation (1024D)
4. **Deep Learning**: Deep hierarchy (64 ‚Üí 128 ‚Üí 256 ‚Üí 512 ‚Üí 4096)

---

## üîÆ Future Improvements

### **Short-term Enhancements**
- [ ] **Data Augmentation**: Rotation, flipping, brightness, elastic deformations
- [ ] **Class Balancing**: Weighted loss, SMOTE, focal loss
- [ ] **Ensemble Methods**: Combine VGGNet + GoogLeNet predictions
- [ ] **Cross-validation**: 5-fold CV for robust evaluation
- [ ] **Hyperparameter Tuning**: Grid search for optimal learning rate, batch size

### **Medium-term Goals**
- [ ] **Modern Architectures**: ResNet-50, EfficientNet, Vision Transformers
- [ ] **Explainability**: Grad-CAM, SHAP, attention maps
- [ ] **Multi-class Classification**: Glioma, meningioma, pituitary tumor types
- [ ] **Segmentation**: Precise tumor boundary detection (U-Net, Mask R-CNN)
- [ ] **3D Analysis**: Volumetric MRI processing instead of 2D patches

### **Long-term Vision**
- [ ] **Clinical Deployment**: Web app for radiologists
- [ ] **Real-time Inference**: Optimize for low-latency prediction
- [ ] **Multi-modal Fusion**: Combine MRI, CT, PET scans
- [ ] **Federated Learning**: Train across hospitals without sharing data
- [ ] **Uncertainty Quantification**: Bayesian deep learning for confidence scores

### **Research Directions**
- [ ] Compare with state-of-the-art medical imaging models
- [ ] Publish performance benchmark on public datasets (BraTS)
- [ ] Investigate few-shot learning for rare tumor types
- [ ] Explore self-supervised pre-training on unlabeled MRI data

---

## üéì Educational Value

This project serves as a **comprehensive learning resource** for understanding:

### **Computer Vision Fundamentals**
‚úÖ Traditional feature extraction (HOG, GLCM, LBP)  
‚úÖ Machine learning classifiers (Random Forest, XGBoost)  
‚úÖ Image preprocessing and patch-based analysis

### **Deep Learning Concepts**
‚úÖ Convolutional Neural Networks (CNNs)  
‚úÖ Transfer learning and fine-tuning  
‚úÖ Architecture exploration (VGGNet, GoogLeNet)  
‚úÖ Training strategies and optimization

### **Medical Imaging Applications**
‚úÖ Real-world problem: Brain tumor detection  
‚úÖ Handling class imbalance in medical data  
‚úÖ Evaluation metrics for healthcare AI  
‚úÖ Clinical deployment considerations

### **Course Alignment**
This project demonstrates the evolution of computer vision techniques:
1. **Phase 1**: Classical methods (foundations)
2. **Phase 2**: Deep learning (modern state-of-the-art)
3. **Comparison**: Understanding trade-offs and when to use each approach

---

## üìÑ License

This project is licensed under the **MIT License** - see below for details:

```
MIT License

Copyright (c) 2024 Shaik Abdus Sattar, Sanhitha

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## ü§ù Contributing

We welcome contributions! Please follow these guidelines:

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/your-feature`
3. **Commit changes**: `git commit -m 'Add some feature'`
4. **Push to branch**: `git push origin feature/your-feature`
5. **Open a Pull Request**

### **Areas for Contribution**
- üêõ Bug fixes
- üìù Documentation improvements
- ‚ú® New features (architectures, visualizations)
- üß™ Additional experiments and benchmarks

---

## üìß Contact

**Project Maintainers**:
- **Shaik Abdus Sattar**: [@Seventie](https://github.com/Seventie)
- **Sanhitha**: [@sanhithaac](https://github.com/sanhithaac)

**Repository**: [github.com/Seventie/BrainTumor-Classification---Computer-Vision](https://github.com/Seventie/BrainTumor-Classification---Computer-Vision)

---

## üôè Acknowledgments

- **ImageNet Dataset**: For pre-trained model weights
- **PyTorch Team**: For the excellent deep learning framework
- **Scikit-learn**: For classical ML implementations
- **OpenCV & Scikit-image**: For image processing utilities
- **Computer Vision Course**: For motivating this comprehensive study

---

<div align="center">

### ‚≠ê **Star this repository if you found it helpful!** ‚≠ê

![Visitors](https://visitor-badge.laobi.icu/badge?page_id=Seventie.BrainTumor-Classification)
![GitHub Stars](https://img.shields.io/github/stars/Seventie/BrainTumor-Classification---Computer-Vision?style=social)
![GitHub Forks](https://img.shields.io/github/forks/Seventie/BrainTumor-Classification---Computer-Vision?style=social)

**Made with ‚ù§Ô∏è for the Computer Vision Community**

</div>
